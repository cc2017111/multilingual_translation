src: zh
tgt: ja
model_arch: ls_transformer_wmt_en_de_big_t2t
encoder_learned_pos: false
decoder_learned_pos: false
data_path: /home/being/PycharmProjects/mRASP-master/experiments/example/data/toy/data/fine-tune/zh2ja
model_dir: /home/being/PycharmProjects/mRASP-master/experiments/example/data/toy/models/fine-tune/transformer_big/zh2ja
pretrain_model_dir: /home/being/PycharmProjects/mRASP-master/experiments/example/data/toy/models/pre-train/transformer_big
finetune_last_model_dir: /home/being/PycharmProjects/mRASP-master/experiments/example/data/toy/models/fine-tune/transformer_big/zh2ja
update_freq: 1
log_interval: 5
save_interval_updates: 1000
max_update: 40000
max_tokens: 8192
max_source_positions: 256
max_target_positions: 256
lr: 5e-4
dropout: 0.2
activation_fn: relu
criterion: ls_label_smoothed_cross_entropy
reset_optimizer: true
reset_lr_scheduler: true
reset_dataloader: true
reset_meters: true
lr_scheduler: inverse_sqrt
weight_decay: 0.0
clip_norm: 0.0
warmup_init_lr: 1e-07
label_smoothing: 0.1
fp16: true
seed: 5
